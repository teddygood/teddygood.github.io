"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[9452],{32536(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/optiver-kaggle","metadata":{"permalink":"/en/projects/optiver-kaggle","source":"@site/i18n/en/docusaurus-plugin-content-blog-projects/optiver-kaggle.md","title":"Optiver \\"Trading at the Close\\" Competition Retrospective","description":"Kaggle Optiver - Trading at the Close Retrospective (Bronze Medal, Top 30%)","date":"2026-02-04T03:19:16.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Optiver \\"Trading at the Close\\" Competition Retrospective","description":"Kaggle Optiver - Trading at the Close Retrospective (Bronze Medal, Top 30%)","role":"Data Scientist","timeline":"Oct 2023 - Dec 2023","stack":["Python","LightGBM","Numba","Pandas"],"category":"Kaggle Competition","image":"/img/projects/optiver-kaggle-header.png","hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Serverless Code Review Assistant on AWS","permalink":"/en/projects/serverless-code-review"}},"content":"{/* truncate */}\\n\\n\\n# Optiver \\"Trading at the Close\\" Competition Retrospective\\n\\n## Motivation\\n\\nIn 2023, while participating in the 4th cohort of the Google ML Bootcamp, one of the completion requirements was to participate in a Kaggle competition. Coincidentally, Optiver was hosting a competition called \\"Trading at the Close,\\" and I chose it because although it was a domain completely unrelated to me and unfamiliar at the time, it seemed interesting. My final submission was LightGBM-based code.\\n\\n---\\n\\n## Competition Overview\\n\\n### Optiver and Closing Auctions\\n\\nOptiver is a global electronic market maker. They provide liquidity to major exchanges worldwide while trading derivatives, equities, ETFs, bonds, and foreign exchange.\\n\\nThe **Nasdaq Closing Cross** takes place daily during the last 10 minutes of the market (3:50\u20134:00 PM ET), during which the official closing price for the day is determined. It is a complex period where investors adjust positions, leading to high volatility, and requires simultaneous processing of traditional order book data and auction data.\\n\\n### Schedule and Prize\\n\\n![Competition Timeline](/img/projects/optiver-timeline.png)\\n\\nThis is a forecasting competition with an active training phase and a second period where models will be run against real market data.\\n\\n**Training Timeline:**\\n- **Sep 20, 2023** - Start Date.\\n- **Dec 13, 2023** - Entry Deadline.\\n- **Dec 13, 2023** - Team Merger Deadline.\\n- **Dec 20, 2023** - Final Submission Deadline.\\n\\nAll deadlines are at 11:59 PM UTC.\\n\\n**Forecasting Timeline:**\\nSelected notebooks are run against real market data with leaderboard updates every two weeks.\\n\\n- **Mar 20, 2024** - Competition End Date\\n- **Total Prize**: $100,000 (1st Place $25,000)\\n\\nUnlike typical Kaggle competitions, this was a **forecasting competition** where models were evaluated on actual market data for about 3 months after submission. The leaderboard was updated every two weeks, and the previous day\'s answers were released daily via `revealed_targets`.\\n\\n### Problem Definition\\n\\nThe task was to predict price movements 60 seconds into the future during the closing auction period.\\n\\n### Target Definition\\n\\nThe target is the change in the WAP (Weighted Average Price) of an individual stock minus the change in the Index WAP, expressed in basis points (bp, 0.01%):\\n\\n$$\\nTarget = \\\\left( \\\\frac{StockWAP_{t+60}}{StockWAP_t} - \\\\frac{IndexWAP_{t+60}}{IndexWAP_t} \\\\right) \\\\times 10000\\n$$\\n\\nIn other words, it is a problem of predicting the \\"relative\\" price change of an individual stock with the overall market movement removed.\\n\\n### Evaluation Metric\\n\\nMean Absolute Error (MAE):\\n\\n$$\\nMAE = \\\\frac{1}{n} \\\\sum_{i=1}^{n} |y_i - x_i|\\n$$\\n\\nWhere:\\n\\n- $n$ is the total number of data points.\\n- $y_i$ is the predicted value for data point i.\\n- $x_i$ is the observed value for data point i.\\n\\n### Data Structure\\n\\nThe training data consists of closing auction snapshots for about 200 stocks over 481 days, totaling about 5 million rows. Key columns are as follows:\\n\\n- `stock_id`: Stock unique identifier\\n- `date_id`: Date identifier (sequential)\\n- `seconds_in_bucket`: Seconds elapsed since auction start\\n- `imbalance_size`: Unmatched volume at current reference_price (USD)\\n- `imbalance_buy_sell_flag`: Imbalance direction: 1 (Buy side), -1 (Sell side), 0 (None)\\n- `reference_price`: Price where matched volume is maximized\\n- `matched_size`: Volume matchable at current reference_price (USD)\\n- `far_price` / `near_price`: Optimal execution price (Auction only / Including continuous orders)\\n- `bid_price` / `ask_price`: Best bid/ask price in non-auction order book\\n- `bid_size` / `ask_size`: Volume of best bid/ask (USD)\\n- `wap`: Weighted average price of non-auction order book ($BidP \\\\times AskS + AskP \\\\times BidS) / (BidS + AskS)$\\n\\n---\\n\\n## My Approach\\n\\n### Feature Engineering\\n\\nI used a total of 124 features in `my-submission.ipynb`. They can be broadly categorized into five groups:\\n\\n**1. Liquidity/Imbalance Features**\\n- `liquidity_imbalance`: Order volume imbalance \u2192 `(bid_size - ask_size) / (bid_size + ask_size)`\\n- `matched_imbalance`: Matched/Imbalance volume ratio \u2192 `(imbalance_size - matched_size) / (imbalance_size + matched_size)`\\n- `size_imbalance`: Simple ratio \u2192 `bid_size / ask_size`\\n- `imbalance_momentum`: Time change of `imbalance_size` per stock divided by `matched_size`\\n\\n**2. Price Spread Features**\\n- `price_spread`: `ask_price - bid_price`\\n- `spread_intensity`: Change in spread per stock\\n- `price_pressure`: `imbalance_size * price_spread`\\n- `market_urgency`: `price_spread * liquidity_imbalance`\\n\\n**3. Price Pair/Triplet Ratio Features**\\n- Imbalance ratios for all pair combinations of 6 prices (`reference_price`, `far_price`, `near_price`, `ask_price`, `bid_price`, `wap`) \u2192 `(P1 - P2) / (P1 + P2)` form\\n- Numba-accelerated triplet imbalance: (max - mid) / (mid - min) ratio of three prices\\n\\n**4. Time Series Features**\\n- 1/2/3/10 lags and pct_change for `matched_size`, `imbalance_size`, `reference_price`, `imbalance_buy_sell_flag`\\n- 1/2/3/10 diffs for `ask_price`, `bid_price`, `ask_size`, `bid_size`, `market_urgency`, `imbalance_momentum`, `size_imbalance`\\n\\n**5. Global Statistics Features**\\n- Median, std, ptp (peak-to-peak) of `bid_size`, `ask_size`, `bid_price`, `ask_price` per stock mapped to `global_*`\\n- Time derivatives: `dow` (day of week), `seconds`, `minute`\\n\\n### Model and Training\\n\\n- **Model**: LightGBM (`objective=\'mae\'`, **Tesla P100 GPU**)\\n- **Hyperparameters**: n_estimators=6000, num_leaves=256, max_depth=11, learning_rate=0.00871, subsample=0.6, colsample_bytree=0.8\\n- **Validation**: Date-based 5-fold CV, with a 5-day purge gap between folds (to prevent time leakage)\\n- **Final Training**: Retrained on full data using the average best_iteration (approx. 5103) of the 5 folds\\n\\n### Inference\\n\\n- Mantained a rolling cache of the last 21 rows per stock to generate features\\n- Equal-weight ensemble of 5 fold models\\n- `zero_sum` post-processing: Subtract weighted average from predictions using **square root of volume (bid+ask size)** as weights. This ensures the market-wide sum is zero.\\n- Clipping: Limited to [-64, 64] range\\n\\n---\\n\\n## Results\\n\\n| Item | Value |\\n|---|---|\\n| 5-fold CV MAE | 5.83 (Per fold: 6.75, 6.12, 5.97, 5.49, 4.80) |\\n| Public/Private LB | 5.4744 |\\n| Rank | 981st / 3,225 Teams (Top 30%) |\\n| Runtime | 2h 35m (P100 GPU) |\\n| Inference Speed | ~0.34s per batch |\\n\\n---\\n\\n## Comparison with Top Solutions\\n\\nAfter the competition, I reviewed top solutions to see what I missed.\\n\\n### 1st Place \u2013 HYD (hydantess)\\n\\n- **Model**: CatBoost(50%) + GRU(30%) + Transformer(20%) Ensemble\\n- **Features**: GRU captured 55-step time-series patterns, Transformer captured cross-sectional relationships among 200 stocks\\n- **Online Learning**: Retrained every 12 days (Total 4 times)\\n- **Weighted zero_sum**: `pred -= (pred * stock_weights).sum() / stock_weights.sum()`\\n- **Sample Weights**: Normalized per-stock volatility with `1.0 / (stock_std + 1e-7)`\\n\\n### 9th Place \u2013 ADAM. (hookman)\\n\\n- **Model**: 3\xd7 XGBoost (different seeds)\\n- **Features**: 157 features, median-scaled volume, MACD, WAP-based target proxy\\n- **Data Weights**: 1.5x weight on last 45 days\\n- **Online Learning**: Retrained on Day N and Day N+30\\n\\n### 14th Place \u2013 Clash Royale (5-person team)\\n\\n- **Model**: LightGBM + CatBoost simple average\\n- **Features**: 193 features, revealed target lags, RSI/MACD/Bollinger Bands\\n- **Online Learning**: Alternating LGB/CAT retraining every 6-9 days\\n- **Key Fix**: Fixed leakage in `mid_price_movement` calculation by grouping by `stock_id`\\n\\n### 15th Place \u2013 O Yuksel (lognorm)\\n\\n- **Model**: 3\xd7 LGB (Offline) + 4\xd7 XGB + 1\xd7 LGB (Online)\\n- **Features**: Revealed target, relative performance vs. market/sector, behavior-based embeddings instead of stock_id\\n- **Post-processing**: Subtracted index-weighted mean instead of simple zero-sum\\n\\n### Common Themes and What I Missed\\n\\n| Top Solution Feature | My Solution |\\n|---|---|\\n| **Online Learning** (Retrain every 6~30 days) | Trained once and fixed |\\n| **Multi-Model Ensemble** (LGB+CAT+XGB or +GRU/Transformer) | LightGBM Single |\\n| **Per-Stock Scaling** (stock_std, median) | used Global statistics only |\\n| **Weighted zero_sum** | Used simple volume sqrt weights |\\n| **Revealed Target Features** | Not used |\\n| **Seed Diversity** (3+ same models) | 5-fold ensemble only |\\n\\n---\\n\\n## Retrospective\\n\\n### What Went Well\\n- Accelerated triplet imbalance calculation with Numba to reduce feature generation time\\n- Prevented time leakage with date-based purge gap\\n- Ran stably in Kaggle environment thanks to memory downcasting\\n\\n### What Could Be Improved\\n- **Did not use Online Learning.** The competition API provided the **previous day\'s target (`revealed_targets`)** at the start of each day. Top rankers did not discard this data but periodically retrained models to reflect the latest market trends. I trained the model once and left it for 3 months, failing to adapt to drift.\\n- **Did not use Sequence Models.** The 1st place solution used GRU for temporal order and Transformer for cross-stock relationships. Tree models alone likely struggled to capture these structural patterns.\\n- **Lack of Per-Stock Normalization.** Scaling by stock_std or median could have corrected for volatility differences between stocks.\\n- **Persistence:** Since this project was for completing the Google ML Bootcamp, I didn\'t persist until the very end of the competition. I wonder if I could have gotten a higher score if I had put in a bit more effort.\\n\\n---\\n\\n## References\\n\\n- [1st place \u2013 HYD](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/hyd-1st-place-solution)\\n- [9th place \u2013 ADAM.](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/adam-9th-place-solution)\\n- [14th place \u2013 Clash Royale](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/clash-royale-14th-place-solution-for-the-optiver-t)\\n- [15th place \u2013 O Yuksel](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/o-yuksel-15th-place-solution)\\n- [Kaggle Data Page](https://www.kaggle.com/competitions/optiver-trading-at-the-close/data)"},{"id":"/serverless-code-review","metadata":{"permalink":"/en/projects/serverless-code-review","source":"@site/projects/serverless-code-review.md","title":"Serverless Code Review Assistant on AWS","description":"Automated code review bot powered by AWS Serverless architecture and Bedrock.","date":"2026-02-04T03:19:16.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Serverless Code Review Assistant on AWS","description":"Automated code review bot powered by AWS Serverless architecture and Bedrock.","role":"Cloud Engineer","timeline":"Jan 2024 - Present","stack":["AWS Lambda","API Gateway","DynamoDB","Bedrock"],"category":"Personal Project","image":"/img/docusaurus.png","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Optiver \\"Trading at the Close\\" Competition Retrospective","permalink":"/en/projects/optiver-kaggle"}},"content":"{/* truncate */}\\n\\n\\n## Overview\\nThe **Serverless Code Review Assistant** is an automated tool designed to streamline the code review process. By leveraging **AWS Serverless** services and **Generative AI** (Amazon Bedrock), it automatically analyzes Pull Requests, detects potential bugs, and suggests refactoring improvements directly in GitHub comments.\\n\\n## Architecture\\n- **Trigger**: GitHub Webhook triggers an event on PR creation/update.\\n- **Compute**: **AWS Lambda** processes the payload and fetches code diffs.\\n- **AI Model**: **Amazon Bedrock** (Claude 3) analyzes the code context.\\n- **Storage**: **Amazon DynamoDB** stores review logs and user feedback.\\n\\n## Key Features\\n- **Automated Feedback**: Instant review comments on new PRs.\\n- **Smart Suggestions**: Context-aware code improvements.\\n- **Serverless**: Fully event-driven and cost-effective architecture."}]}}')}}]);