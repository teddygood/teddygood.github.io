"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[9452],{32536(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/doc-to-rag-benchmark","metadata":{"permalink":"/en/projects/doc-to-rag-benchmark","source":"@site/i18n/en/docusaurus-plugin-content-blog-projects/doc-to-rag-benchmark.md","title":"Doc-to-RAG Benchmark","description":"From Doc-to-Text to evaluable RAG data | Retrospective on a PDF-based Doc-to-RAG Benchmark project from the Meta Llama Academy workshop","date":"2026-02-20T15:21:19.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Doc-to-RAG Benchmark","description":"From Doc-to-Text to evaluable RAG data | Retrospective on a PDF-based Doc-to-RAG Benchmark project from the Meta Llama Academy workshop","role":"AI Engineer","timeline":"Sep 2025 - Oct 2025","stack":["LangGraph","Upstage API","Solar Pro 2","Llama 3.2","PDF Parsing","QLoRA"],"category":"Team Project","image":"/img/projects/doc-to-rag-benchmark/experience/experience-01-expanded.png","hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Optiver \\"Trading at the Close\\" Competition Retrospective","permalink":"/en/projects/optiver-kaggle"}},"content":"{/* truncate */}\\n\\n# Doc-to-RAG Benchmark\\n\\n## Why We Started This Project\\n\\nEarly in the workshop, the biggest problem became obvious. To compare RAG performance, we first needed \\"comparable data.\\" But for PDF documents, each parser produced different outputs, so even the same question had unstable evaluation baselines.\\n\\nWithin the team, we aligned on one direction: not \\"showing off a model,\\" but \\"fixing the evaluation baseline.\\" We believed the pipeline had to connect document extraction, QA generation, judge validation, and dataset storage end-to-end to make experiments reproducible.\\n\\n## Team Structure and Roles\\n\\nThe team was organized as follows:\\n\\n1. Project lead: direction, priorities, and schedule management\\n2. Frontend + backend: demo and service integration\\n3. Multi-agent implementation: parsing/refinement graph design\\n4. Data/evaluation pipeline: extraction-text benchmark + LLM Judge + Hugging Face upload\\n5. Me: QLoRA-based fine-tuning experiment design and operation\\n\\nMy biggest focus was role #5. I concentrated on whether fine-tuning produced meaningful gains for the overall project and how to keep those results in an operationally usable form.\\n\\n## Detailed Role Breakdown (Beyond My Own Role)\\n\\nAs emphasized in our initial draft, this project was not completed by one person\'s model experiments. It worked because multiple axes were integrated. So it is important to summarize non-my-role contributions from an implementation point of view too.\\n\\n### 1) Project Lead: Fixing Direction and Decision Framework\\n\\nThe project lead did more than schedule management. They fixed what \\"success\\" meant.\\n\\n1. Defined the goal as \\"building a reproducible Doc-to-RAG pipeline,\\" not \\"finding the highest-score model\\"\\n2. Set priorities by bottleneck order (extraction -> validation -> refinement -> evaluation), not feature listing\\n3. Unified demo messaging around operational viability (balance of cost/speed/quality), not just benchmark numbers\\n\\nBecause of this alignment, even though each member built different modules, final outputs converged toward one direction.\\n\\n### 2) Frontend + Backend: Connecting the Demo into a Real Execution Flow\\n\\nThe frontend/backend owner tied each pipeline stage into one visible flow.\\n\\n1. Visualized each stage from input document to extraction output, validation status, refinement result, and final selection result\\n2. Delivered backend artifacts (JSON/logs) in reusable formats for frontend rendering\\n3. Built the demo so failure and success cases could be compared in the same scenario flow\\n\\nAs a result, our presentation could show not \\"this model is good,\\" but \\"how the system makes decisions.\\"\\n\\n### 3) Multi-Agent Implementation: Making the Parsing/Refinement Graph Operable\\n\\nThe multi-agent owner split the system into two LangGraph-based axes:\\n\\n1. OCR/parsing strategy-selection graph: base extraction -> validation -> judge -> final selection\\n2. Refine graph: determine if refinement is needed -> refine only necessary pages -> re-validate\\n\\nKey implementation points were page-level fallback and early stop.\\n\\n1. Apply post-processing only to failed pages, not to the entire document\\n2. Stop early if there is no improvement\\n3. Use a composite score that includes both quality and throughput for final selection\\n\\nWithout this design, accuracy might improve, but costs and latency would rise sharply.\\n\\n### 4) Data/Evaluation Pipeline: Turning Benchmarking into a Reusable Asset\\n\\nThe data/evaluation owner converted extraction outputs from \\"one-off results\\" into \\"reusable experimental assets.\\"\\n\\n1. Built a QA generation pipeline on top of extracted text\\n2. Separated LLM Judge usage between validation and evaluation stages to reduce role confusion\\n3. Stored outputs in CSV/JSON/dataset formats for direct reuse in future experiments\\n4. Connected to Hugging Face upload so others could reproduce with the same format\\n\\nBecause of this axis, even fine-tuning results could be compared on a fixed baseline instead of being one-shot outcomes.\\n\\n## Project Design: How We Structured the End-to-End Flow\\n\\nAt first we thought, \\"Picking one good parser is enough,\\" but in practice it was more complex.  \\nWhen extraction quality fluctuates, QA generation fluctuates. When QA quality fluctuates, LLM Judge outcomes also fluctuate.  \\nSo we designed this project by pipeline units, not by isolated features.\\n\\n### 1) Why We Split the Codebase into Three Repositories\\n\\nTo balance experiment speed and operational stability, we separated repositories by role:\\n\\n1. `llamacpp`: experimentation axis for rapid fine-tuning/inference/evaluation loops\\n2. `Doc-to-RAG-Benchmark`: product-facing flow, demo narrative, and evaluation standard axis\\n3. `Doc-to-text_Parsing_Agent`: implementation axis for OCR/parsing strategy selection + refine processing\\n\\nThis separation reduced conflicts between model experiments and service/agent code. Likewise, parser logic changes could be validated independently without breaking the fine-tuning loop.\\n\\n### 2) Key Design Points in Parsing/Refinement\\n\\nThe core was to avoid expensive full-document processing.\\n\\n1. Run multiple parsers in parallel at base extraction to produce candidates\\n2. Determine pass/fail in validation\\n3. Apply fallback/refinement only to failed pages\\n4. Re-validate and stop if no improvement is observed\\n\\nThis approach improved quality while controlling API cost and processing time.\\n\\n### 3) Why Evaluation Was Not Just One Score\\n\\nIn this project, evaluation was not just for leaderboard ranking. It was for defining a baseline for the next experiment.\\n\\n1. Quality: extraction accuracy, answer consistency, preservation of noise/table/sentence structure\\n2. Speed: per-document processing time and bottleneck zones\\n3. Cost: external LLM call frequency and per-page cost\\n\\nIn other words, the criterion was not \\"did this run look good,\\" but \\"is this reproducible next time.\\"\\n\\nFor production evaluation, we fixed LLM Judge weights as follows:\\n\\n1. `S_read` 25%\\n2. `S_sent` 25%\\n3. `S_noise` 15%\\n4. `S_table` 25%\\n5. `S_fig` 10%\\n\\nIn final selection, we used 80% quality score + 20% processing speed for decision-making.\\n\\n## My Core Contribution: Fine-Tuning Experiment Design and Operation\\n\\n### 1) I Fixed the Training Loop First\\n\\nThe first thing I built was not a one-shot script, but a repeatable experiment loop.\\n\\n1. Base model: `meta-llama/Llama-3.2-1B-Instruct`\\n2. Comparison model: `meta-llama/Llama-3.2-3B-Instruct`\\n2. Data: KorQuAD v1 normalization + validation sampling\\n3. Method: QLoRA (4bit NF4)\\n4. Comparison: side-by-side inference between base and fine-tuned model\\n5. Evaluation: fixed on EM/F1\\n6. Deployment validation: merged LoRA -> GGUF conversion -> `llama.cpp` inference\\n\\nDuring the project, I ran both 1B and 3B tracks in parallel.  \\n1B was useful for quick iteration and parameter exploration, while 3B was used as a comparison axis to check quality ceilings and improvement gaps.  \\nIn the final phase, we decided configurations not only by quality but also by processing time and resource cost.\\n\\n### 2) I Prioritized \\"Sustainable Experiments\\" Over \\"One Good Run\\"\\n\\nPerformance numbers alone often collapse in the next iteration, so I kept operational metrics together.\\n\\n1. Controlled training-time ceiling with `TimeBudgetCallback`\\n2. Logged step-level wall time/CUDA time/memory usage as CSV\\n3. Automated LoRA merge at training completion in one script\\n\\nThis made it possible to verify not only \\"performance improved,\\" but also \\"why it improved\\" in later runs.\\n\\n### 3) I Kept the Fine-Tuning Target Narrow\\n\\nThe goal was not generic conversational quality. It was to improve answer consistency and evidence retrieval in document-grounded QA.  \\nSo we kept data format, prompt template, and evaluation routine stable.\\n\\n## How Fine-Tuning Connected to the Full Pipeline\\n\\nMy key takeaway was simple: in RAG, improving one component alone does not guarantee end-to-end improvement.\\n\\n1. If parsing quality is low, retrieval becomes unstable.\\n2. If retrieval is unstable, hallucinations increase even with a stronger generator.\\n3. So fine-tuning should be treated as a final-stage stabilizer and operated together with upstream quality standards.\\n\\nIn short, fine-tuning was not a standalone solution. It served to stabilize final response quality once parsing/evaluation baselines were already organized.\\n\\n## Service Flow (Based on README)\\n\\n### Overall Architecture\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/doc1-left.png\\"\\n  alt=\\"Doc-to-RAG Overview (1)\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"0 auto\\" }}\\n/>\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/doc1-right.png\\"\\n  alt=\\"Doc-to-RAG Overview (2)\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"16px auto 0\\" }}\\n/>\\n\\nUsing this diagram, we first fixed role boundaries within the team. The goal was to treat document parsing optimization and benchmark automation as one pipeline, not disconnected tasks.\\n\\n### Problem Definition and Approach\\n\\n![Problem Definition](/img/projects/doc-to-rag-benchmark/doc2.png)\\n\\nThe key issue was that each document had different structure, which caused large extraction-quality variance even for the same question.  \\nSo instead of fixing one parser, we designed a strategy-selection architecture based on document characteristics.\\n\\n### OCR/Parsing Strategy Selection System\\n\\n![OCR/Parsing Strategy Selection](/img/projects/doc-to-rag-benchmark/doc3.png)\\n\\nThe critical operational principle here was page-level fallback. We corrected quality by applying post-processing only to failed pages and avoided heavy processing across all pages.\\n\\n### Document Refinement (Refine) System\\n\\n![Document Refinement (Refine)](/img/projects/doc-to-rag-benchmark/doc4.png)\\n\\nWe controlled costs with the same principle in refinement. We first judged on-device, then sent only `Need Refine` cases to Solar.\\n\\n### End-to-End Pipeline\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/doc5.png\\"\\n  alt=\\"End-to-End Pipeline\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"0 auto\\" }}\\n/>\\n\\nWe accumulated extraction and evaluation outputs on the same baseline, so results became reusable assets for future experiments, not one-time outputs.\\n\\n### Evaluation Perspectives and Result Summary\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/doc6.png\\"\\n  alt=\\"Evaluation Perspective 1\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"0 auto\\" }}\\n/>\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/doc7.png\\"\\n  alt=\\"Evaluation Perspective 2\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"16px auto 0\\" }}\\n/>\\n\\nInstead of concluding from one score, we looked at quality, speed, and cost together. Fixing this criterion helped the team align quickly when interpreting fine-tuning outcomes.\\n\\n## Demo Screens\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/domo1.png\\"\\n  alt=\\"Demo Screen 1\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"0 auto\\" }}\\n/>\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/domo2.png\\"\\n  alt=\\"Demo Screen 2\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"16px auto 0\\" }}\\n/>\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/domo3.png\\"\\n  alt=\\"Demo Screen 3\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"16px auto 0\\" }}\\n/>\\n\\n<img\\n  src=\\"/img/projects/doc-to-rag-benchmark/domo4.png\\"\\n  alt=\\"Demo Screen 4\\"\\n  style={{ width: \\"100%\\", maxWidth: \\"1100px\\", height: \\"auto\\", display: \\"block\\", margin: \\"16px auto 0\\" }}\\n/>\\n\\n![Demo Screen 5](/img/projects/doc-to-rag-benchmark/domo5.png)\\n\\nThe demo section was designed to show how \\"selection -> validation -> refinement -> evaluation\\" actually appears in operation.  \\nIn presentation, it was useful for explaining reproducible operational flow, not only model performance numbers.\\n\\n### The Experience\\n\\n<div\\n  style={{\\n    display: \\"grid\\",\\n    gridTemplateColumns: \\"repeat(auto-fit, minmax(220px, 1fr))\\",\\n    gap: \\"12px\\",\\n    alignItems: \\"start\\",\\n  }}\\n>\\n  <img\\n    src=\\"/img/projects/doc-to-rag-benchmark/experience/experience-01.jpeg\\"\\n    alt=\\"The experience 1\\"\\n    style={{ width: \\"100%\\", height: \\"auto\\", display: \\"block\\", borderRadius: \\"8px\\" }}\\n  />\\n  <img\\n    src=\\"/img/projects/doc-to-rag-benchmark/experience/experience-02.jpeg\\"\\n    alt=\\"The experience 2\\"\\n    style={{ width: \\"100%\\", height: \\"auto\\", display: \\"block\\", borderRadius: \\"8px\\" }}\\n  />\\n  <img\\n    src=\\"/img/projects/doc-to-rag-benchmark/experience/experience-03.jpeg\\"\\n    alt=\\"The experience 3\\"\\n    style={{ width: \\"100%\\", height: \\"auto\\", display: \\"block\\", borderRadius: \\"8px\\" }}\\n  />\\n  <img\\n    src=\\"/img/projects/doc-to-rag-benchmark/experience/experience-04.jpeg\\"\\n    alt=\\"The experience 4\\"\\n    style={{ width: \\"100%\\", height: \\"auto\\", display: \\"block\\", borderRadius: \\"8px\\" }}\\n  />\\n  <img\\n    src=\\"/img/projects/doc-to-rag-benchmark/experience/experience-05.jpeg\\"\\n    alt=\\"The experience 5\\"\\n    style={{ width: \\"100%\\", height: \\"auto\\", display: \\"block\\", borderRadius: \\"8px\\" }}\\n  />\\n  <img\\n    src=\\"/img/projects/doc-to-rag-benchmark/experience/experience-06.jpeg\\"\\n    alt=\\"The experience 6\\"\\n    style={{ width: \\"100%\\", height: \\"auto\\", display: \\"block\\", borderRadius: \\"8px\\" }}\\n  />\\n</div>\\n\\n## What We Confirmed Through This Project\\n\\nThe clearest takeaway was this: in RAG systems, \\"good evaluation data and selection criteria\\" improves stability more reliably than \\"one strong model.\\"\\n\\nFrom my role\'s perspective, fixing the experiment loop and preserving reproducible outputs mattered more for long-term iteration speed than raw fine-tuning gains alone.\\n\\n## What I Want to Improve Next\\n\\n1. Automatic selection of domain-specific judge prompts\\n2. Consistent benchmark output in Hugging Face-compatible format\\n3. Continuous regression tests for parsing failures and refinement side effects"},{"id":"/optiver-kaggle","metadata":{"permalink":"/en/projects/optiver-kaggle","source":"@site/i18n/en/docusaurus-plugin-content-blog-projects/optiver-kaggle.md","title":"Optiver \\"Trading at the Close\\" Competition Retrospective","description":"Kaggle Optiver - Trading at the Close Retrospective (Bronze Medal, Top 30%)","date":"2026-02-20T15:21:19.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Optiver \\"Trading at the Close\\" Competition Retrospective","description":"Kaggle Optiver - Trading at the Close Retrospective (Bronze Medal, Top 30%)","role":"Data Scientist","timeline":"Oct 2023 - Dec 2023","stack":["Python","LightGBM","Numba","Pandas"],"category":"Kaggle Competition","image":"/img/projects/optiver-kaggle/optiver-kaggle-header-aspect-v2.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Doc-to-RAG Benchmark","permalink":"/en/projects/doc-to-rag-benchmark"},"nextItem":{"title":"Pyodide","permalink":"/en/projects/pyodide"}},"content":"{/* truncate */}\\n\\n\\n# Optiver \\"Trading at the Close\\" Competition Retrospective\\n\\n## Motivation\\n\\nIn 2023, while participating in the 4th cohort of the Google ML Bootcamp, one of the completion requirements was to participate in a Kaggle competition. Coincidentally, Optiver was hosting a competition called \\"Trading at the Close,\\" and I chose it because although it was a domain completely unrelated to me and unfamiliar at the time, it seemed interesting. My final submission was LightGBM-based code.\\n\\n---\\n\\n## Competition Overview\\n\\n### Optiver and Closing Auctions\\n\\nOptiver is a global electronic market maker. They provide liquidity to major exchanges worldwide while trading derivatives, equities, ETFs, bonds, and foreign exchange.\\n\\nThe **Nasdaq Closing Cross** takes place daily during the last 10 minutes of the market (3:50\u20134:00 PM ET), during which the official closing price for the day is determined. It is a complex period where investors adjust positions, leading to high volatility, and requires simultaneous processing of traditional order book data and auction data.\\n\\n### Schedule and Prize\\n\\n![Competition Timeline](/img/projects/optiver-kaggle/optiver-timeline.png)\\n\\nThis is a forecasting competition with an active training phase and a second period where models will be run against real market data.\\n\\n**Training Timeline:**\\n- **Sep 20, 2023** - Start Date.\\n- **Dec 13, 2023** - Entry Deadline.\\n- **Dec 13, 2023** - Team Merger Deadline.\\n- **Dec 20, 2023** - Final Submission Deadline.\\n\\nAll deadlines are at 11:59 PM UTC.\\n\\n**Forecasting Timeline:**\\nSelected notebooks are run against real market data with leaderboard updates every two weeks.\\n\\n- **Mar 20, 2024** - Competition End Date\\n- **Total Prize**: $100,000 (1st Place $25,000)\\n\\nUnlike typical Kaggle competitions, this was a **forecasting competition** where models were evaluated on actual market data for about 3 months after submission. The leaderboard was updated every two weeks, and the previous day\'s answers were released daily via `revealed_targets`.\\n\\n### Problem Definition\\n\\nThe task was to predict price movements 60 seconds into the future during the closing auction period.\\n\\n### Target Definition\\n\\nThe target is the change in the WAP (Weighted Average Price) of an individual stock minus the change in the Index WAP, expressed in basis points (bp, 0.01%):\\n\\n$$\\nTarget = \\\\left( \\\\frac{StockWAP_{t+60}}{StockWAP_t} - \\\\frac{IndexWAP_{t+60}}{IndexWAP_t} \\\\right) \\\\times 10000\\n$$\\n\\nIn other words, it is a problem of predicting the \\"relative\\" price change of an individual stock with the overall market movement removed.\\n\\n### Evaluation Metric\\n\\nMean Absolute Error (MAE):\\n\\n$$\\nMAE = \\\\frac{1}{n} \\\\sum_{i=1}^{n} |y_i - x_i|\\n$$\\n\\nWhere:\\n\\n- $n$ is the total number of data points.\\n- $y_i$ is the predicted value for data point i.\\n- $x_i$ is the observed value for data point i.\\n\\n### Data Structure\\n\\nThe training data consists of closing auction snapshots for about 200 stocks over 481 days, totaling about 5 million rows. Key columns are as follows:\\n\\n- `stock_id`: Stock unique identifier\\n- `date_id`: Date identifier (sequential)\\n- `seconds_in_bucket`: Seconds elapsed since auction start\\n- `imbalance_size`: Unmatched volume at current reference_price (USD)\\n- `imbalance_buy_sell_flag`: Imbalance direction: 1 (Buy side), -1 (Sell side), 0 (None)\\n- `reference_price`: Price where matched volume is maximized\\n- `matched_size`: Volume matchable at current reference_price (USD)\\n- `far_price` / `near_price`: Optimal execution price (Auction only / Including continuous orders)\\n- `bid_price` / `ask_price`: Best bid/ask price in non-auction order book\\n- `bid_size` / `ask_size`: Volume of best bid/ask (USD)\\n- `wap`: Weighted average price of non-auction order book ($BidP \\\\times AskS + AskP \\\\times BidS) / (BidS + AskS)$\\n\\n---\\n\\n## My Approach\\n\\n### Feature Engineering\\n\\nI used a total of 124 features in `my-submission.ipynb`. They can be broadly categorized into five groups:\\n\\n**1. Liquidity/Imbalance Features**\\n- `liquidity_imbalance`: Order volume imbalance \u2192 `(bid_size - ask_size) / (bid_size + ask_size)`\\n- `matched_imbalance`: Matched/Imbalance volume ratio \u2192 `(imbalance_size - matched_size) / (imbalance_size + matched_size)`\\n- `size_imbalance`: Simple ratio \u2192 `bid_size / ask_size`\\n- `imbalance_momentum`: Time change of `imbalance_size` per stock divided by `matched_size`\\n\\n**2. Price Spread Features**\\n- `price_spread`: `ask_price - bid_price`\\n- `spread_intensity`: Change in spread per stock\\n- `price_pressure`: `imbalance_size * price_spread`\\n- `market_urgency`: `price_spread * liquidity_imbalance`\\n\\n**3. Price Pair/Triplet Ratio Features**\\n- Imbalance ratios for all pair combinations of 6 prices (`reference_price`, `far_price`, `near_price`, `ask_price`, `bid_price`, `wap`) \u2192 `(P1 - P2) / (P1 + P2)` form\\n- Numba-accelerated triplet imbalance: (max - mid) / (mid - min) ratio of three prices\\n\\n**4. Time Series Features**\\n- 1/2/3/10 lags and pct_change for `matched_size`, `imbalance_size`, `reference_price`, `imbalance_buy_sell_flag`\\n- 1/2/3/10 diffs for `ask_price`, `bid_price`, `ask_size`, `bid_size`, `market_urgency`, `imbalance_momentum`, `size_imbalance`\\n\\n**5. Global Statistics Features**\\n- Median, std, ptp (peak-to-peak) of `bid_size`, `ask_size`, `bid_price`, `ask_price` per stock mapped to `global_*`\\n- Time derivatives: `dow` (day of week), `seconds`, `minute`\\n\\n### Model and Training\\n\\n- **Model**: LightGBM (`objective=\'mae\'`, **Tesla P100 GPU**)\\n- **Hyperparameters**: n_estimators=6000, num_leaves=256, max_depth=11, learning_rate=0.00871, subsample=0.6, colsample_bytree=0.8\\n- **Validation**: Date-based 5-fold CV, with a 5-day purge gap between folds (to prevent time leakage)\\n- **Final Training**: Retrained on full data using the average best_iteration (approx. 5103) of the 5 folds\\n\\n### Inference\\n\\n- Mantained a rolling cache of the last 21 rows per stock to generate features\\n- Equal-weight ensemble of 5 fold models\\n- `zero_sum` post-processing: Subtract weighted average from predictions using **square root of volume (bid+ask size)** as weights. This ensures the market-wide sum is zero.\\n- Clipping: Limited to [-64, 64] range\\n\\n---\\n\\n## Results\\n\\n| Item | Value |\\n|---|---|\\n| 5-fold CV MAE | 5.83 (Per fold: 6.75, 6.12, 5.97, 5.49, 4.80) |\\n| Public/Private LB | 5.4744 |\\n| Rank | 981st / 3,225 Teams (Top 30%) |\\n| Runtime | 2h 35m (P100 GPU) |\\n| Inference Speed | ~0.34s per batch |\\n\\n---\\n\\n## Comparison with Top Solutions\\n\\nAfter the competition, I reviewed top solutions to see what I missed.\\n\\n### 1st Place \u2013 HYD (hydantess)\\n\\n- **Model**: CatBoost(50%) + GRU(30%) + Transformer(20%) Ensemble\\n- **Features**: GRU captured 55-step time-series patterns, Transformer captured cross-sectional relationships among 200 stocks\\n- **Online Learning**: Retrained every 12 days (Total 4 times)\\n- **Weighted zero_sum**: `pred -= (pred * stock_weights).sum() / stock_weights.sum()`\\n- **Sample Weights**: Normalized per-stock volatility with `1.0 / (stock_std + 1e-7)`\\n\\n### 9th Place \u2013 ADAM. (hookman)\\n\\n- **Model**: 3\xd7 XGBoost (different seeds)\\n- **Features**: 157 features, median-scaled volume, MACD, WAP-based target proxy\\n- **Data Weights**: 1.5x weight on last 45 days\\n- **Online Learning**: Retrained on Day N and Day N+30\\n\\n### 14th Place \u2013 Clash Royale (5-person team)\\n\\n- **Model**: LightGBM + CatBoost simple average\\n- **Features**: 193 features, revealed target lags, RSI/MACD/Bollinger Bands\\n- **Online Learning**: Alternating LGB/CAT retraining every 6-9 days\\n- **Key Fix**: Fixed leakage in `mid_price_movement` calculation by grouping by `stock_id`\\n\\n### 15th Place \u2013 O Yuksel (lognorm)\\n\\n- **Model**: 3\xd7 LGB (Offline) + 4\xd7 XGB + 1\xd7 LGB (Online)\\n- **Features**: Revealed target, relative performance vs. market/sector, behavior-based embeddings instead of stock_id\\n- **Post-processing**: Subtracted index-weighted mean instead of simple zero-sum\\n\\n### Common Themes and What I Missed\\n\\n| Top Solution Feature | My Solution |\\n|---|---|\\n| **Online Learning** (Retrain every 6~30 days) | Trained once and fixed |\\n| **Multi-Model Ensemble** (LGB+CAT+XGB or +GRU/Transformer) | LightGBM Single |\\n| **Per-Stock Scaling** (stock_std, median) | used Global statistics only |\\n| **Weighted zero_sum** | Used simple volume sqrt weights |\\n| **Revealed Target Features** | Not used |\\n| **Seed Diversity** (3+ same models) | 5-fold ensemble only |\\n\\n---\\n\\n## Retrospective\\n\\n### What Went Well\\n- Accelerated triplet imbalance calculation with Numba to reduce feature generation time\\n- Prevented time leakage with date-based purge gap\\n- Ran stably in Kaggle environment thanks to memory downcasting\\n\\n### What Could Be Improved\\n- **Did not use Online Learning.** The competition API provided the **previous day\'s target (`revealed_targets`)** at the start of each day. Top rankers did not discard this data but periodically retrained models to reflect the latest market trends. I trained the model once and left it for 3 months, failing to adapt to drift.\\n- **Did not use Sequence Models.** The 1st place solution used GRU for temporal order and Transformer for cross-stock relationships. Tree models alone likely struggled to capture these structural patterns.\\n- **Lack of Per-Stock Normalization.** Scaling by stock_std or median could have corrected for volatility differences between stocks.\\n- **Persistence:** Since this project was for completing the Google ML Bootcamp, I didn\'t persist until the very end of the competition. I wonder if I could have gotten a higher score if I had put in a bit more effort.\\n\\n---\\n\\n## References\\n\\n- [1st place \u2013 HYD](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/hyd-1st-place-solution)\\n- [9th place \u2013 ADAM.](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/adam-9th-place-solution)\\n- [14th place \u2013 Clash Royale](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/clash-royale-14th-place-solution-for-the-optiver-t)\\n- [15th place \u2013 O Yuksel](https://www.kaggle.com/competitions/optiver-trading-at-the-close/writeups/o-yuksel-15th-place-solution)\\n- [Kaggle Data Page](https://www.kaggle.com/competitions/optiver-trading-at-the-close/data)"},{"id":"/pyodide","metadata":{"permalink":"/en/projects/pyodide","source":"@site/i18n/en/docusaurus-plugin-content-blog-projects/pyodide.md","title":"Pyodide","description":"Implemented `pyodide clean recipes` CLI and enabled WASM SIMD support for OpenBLAS.","date":"2026-02-20T15:21:19.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Pyodide","description":"Implemented `pyodide clean recipes` CLI and enabled WASM SIMD support for OpenBLAS.","role":"Contributor","timeline":"Ongoing","stack":["Python","WebAssembly","C++"],"category":"Open Source","image":"/img/projects/pyodide/logo-pyodide-repositioned.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Optiver \\"Trading at the Close\\" Competition Retrospective","permalink":"/en/projects/optiver-kaggle"},"nextItem":{"title":"Serverless Code Review Assistant on AWS","permalink":"/en/projects/serverless-code-review"}},"content":"{/* truncate */}\\n\\n# Pyodide\\n\\nHere is the story of my contributions to Pyodide, focusing on build system improvements and performance optimizations via WASM SIMD.\\n\\n## Build System Improvements: `pyodide clean recipes`\\n\\nWhile working with the Pyodide build system, I noticed a friction point: there was no built-in way to clean up downloaded source files and intermediate build artifacts. Developers had to manually delete directories, which was error-prone and tedious.\\n\\nAddressing this ([Issue #218](https://github.com/pyodide/pyodide-build/issues/218)), I implemented a comprehensive cleanup solution in two stages:\\n\\n1.  **Core Logic Implementation ([PR #223](https://github.com/pyodide/pyodide-build/pull/223))**: I implemented cleanup logic in `pyodide-build` to intelligently remove source layouts and build directories while preserving the `dist` folder (where built wheels reside) by default.\\n2.  **CLI Integration ([PR #254](https://github.com/pyodide/pyodide-build/pull/254))**: I provided this functionality through a new CLI command `pyodide clean recipes`. This allowed users to clean specific packages or the entire workspace with simple flags, significantly improving the developer experience for package maintainers.\\n\\n## Performance Optimization: Enabling WASM SIMD\\n\\nPyodide\'s performance is a critical factor for data science in the browser. Emscripten supports WebAssembly SIMD (Single Instruction, Multiple Data) to vectorize C/C++ code, but it wasn\'t clear if this ecosystem was ready for prime time within Pyodide. I led the initiative to validate and enable SIMD support ([Issue #5855](https://github.com/pyodide/pyodide/issues/5855)).\\n\\n### Phase 1: Validation ([PR #5880](https://github.com/pyodide/pyodide/pull/5880))\\nBefore optimizing complex packages, I needed to prove that SIMD intrinsics worked correctly in the Pyodide environment. I created a new test package, `test-simd`, which validated two critical compilation paths:\\n*   Direct usage of `wasm_simd128.h` intrinsics.\\n*   Emscripten\'s translation of SSE/AVX intrinsics to WASM SIMD.\\n\\n### Phase 2: Optimizing OpenBLAS ([PR #5960](https://github.com/pyodide/pyodide/pull/5960))\\nWith validation complete, I moved to a real-world use case: **OpenBLAS**, the foundational library for NumPy and SciPy. I identified that the existing OpenBLAS build was not leveraging SIMD ([Issue #5948](https://github.com/pyodide/pyodide/issues/5948)).\\n\\nI implemented a new build recipe, `libopenblas-simd`, compiling with `-msimd128`. To quantify the impact, I wrote a benchmarking suite (`test-openblas-simd`) comparing scalar vs. SIMD performance for key operations like Dot Product (`sdot`) and Matrix Multiplication (`sgemm`). This work contributed to significant performance gains in scientific computing on the web.\\n\\n\\n## Impact and Achievement\\n\\n### Pyodide\\n\\n#### Release 0.29\\n\\nThe WASM SIMD support and optimization work was contributed to the `pyodide` main repository and included in the 0.29 release. You can also find details in the [Pyodide 0.29 Release Blog](https://blog.pyodide.org/posts/0.29-release/).\\n\\n![SIMD Support Exploration](/img/projects/pyodide/simd-exploration.png)\\n\\n### Pyodide-build\\nMy contributions have been accepted and released in `pyodide-build` versions 0.30.8, 0.30.9, and 0.31.0.\\n\\n#### Release 0.30.8\\nMarking my first contribution to the project.\\n![Release 0.30.8](/img/projects/pyodide/release-0.30.8.png)\\n\\n#### Release 0.30.9\\nIntroduction of the `pyodide clean recipes` CLI.\\n![Release 0.30.9](/img/projects/pyodide/release-0.30.9.png)\\n\\n#### Release 0.31.0\\nFurther refactoring and improvements.\\n![Release 0.31.0](/img/projects/pyodide/release-0.31.0.png)\\n\\n## Summary of Contributions\\n\\n| Project | Pull Request | Description | Status |\\n| :--- | :--- | :--- | :--- |\\n| [pyodide-build](https://github.com/pyodide/pyodide-build) | [PR #223](https://github.com/pyodide/pyodide-build/pull/223) | Implement recipe cleanup logic | Merged |\\n| [pyodide-build](https://github.com/pyodide/pyodide-build) | [PR #254](https://github.com/pyodide/pyodide-build/pull/254) | Add CLI command `pyodide clean recipes` | Merged |\\n| [pyodide](https://github.com/pyodide/pyodide) | [PR #5880](https://github.com/pyodide/pyodide/pull/5880) | Add `test-simd` package to validate WASM/SSE/AVX intrinsics | Merged |\\n| [pyodide](https://github.com/pyodide/pyodide) | [PR #5960](https://github.com/pyodide/pyodide/pull/5960) | Enable SIMD for OpenBLAS (WASM) | Merged |"},{"id":"/serverless-code-review","metadata":{"permalink":"/en/projects/serverless-code-review","source":"@site/projects/serverless-code-review.md","title":"Serverless Code Review Assistant on AWS","description":"Automated code review bot powered by AWS Serverless architecture and Bedrock.","date":"2026-02-20T15:21:19.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Serverless Code Review Assistant on AWS","description":"Automated code review bot powered by AWS Serverless architecture and Bedrock.","role":"Cloud Engineer","timeline":"Jan 2024 - Present","stack":["AWS Lambda","API Gateway","DynamoDB","Bedrock"],"category":"Personal Project","image":"/img/docusaurus.png","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Pyodide","permalink":"/en/projects/pyodide"}},"content":"{/* truncate */}\\n\\n\\n## Overview\\nThe **Serverless Code Review Assistant** is an automated tool designed to streamline the code review process. By leveraging **AWS Serverless** services and **Generative AI** (Amazon Bedrock), it automatically analyzes Pull Requests, detects potential bugs, and suggests refactoring improvements directly in GitHub comments.\\n\\n## Architecture\\n- **Trigger**: GitHub Webhook triggers an event on PR creation/update.\\n- **Compute**: **AWS Lambda** processes the payload and fetches code diffs.\\n- **AI Model**: **Amazon Bedrock** (Claude 3) analyzes the code context.\\n- **Storage**: **Amazon DynamoDB** stores review logs and user feedback.\\n\\n## Key Features\\n- **Automated Feedback**: Instant review comments on new PRs.\\n- **Smart Suggestions**: Context-aware code improvements.\\n- **Serverless**: Fully event-driven and cost-effective architecture."}]}}')}}]);