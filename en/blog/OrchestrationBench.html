<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">[Paper Review] OrchestrationBench: LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios @ Chanho Lee</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://teddygood.github.io/en/blog/OrchestrationBench"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="ko_KR"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="[Paper Review] OrchestrationBench: LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios @ Chanho Lee"><meta data-rh="true" name="description" content="Paper review of LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios"><meta data-rh="true" property="og:description" content="Paper review of LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios"><meta data-rh="true" name="keywords" content="orchestration,agent,llm,benchmark,planning"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2026-02-04T14:57:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/teddygood"><meta data-rh="true" property="article:tag" content="Paper Review,LLM,Agent"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://teddygood.github.io/en/blog/OrchestrationBench"><link data-rh="true" rel="alternate" href="https://teddygood.github.io/blog/OrchestrationBench" hreflang="ko-KR"><link data-rh="true" rel="alternate" href="https://teddygood.github.io/en/blog/OrchestrationBench" hreflang="en"><link data-rh="true" rel="alternate" href="https://teddygood.github.io/blog/OrchestrationBench" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://3GO7VFCZS7-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://teddygood.github.io/en/blog/OrchestrationBench","mainEntityOfPage":"https://teddygood.github.io/en/blog/OrchestrationBench","url":"https://teddygood.github.io/en/blog/OrchestrationBench","headline":"[Paper Review] OrchestrationBench: LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios","name":"[Paper Review] OrchestrationBench: LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios","description":"Paper review of LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios","datePublished":"2026-02-04T14:57:00.000Z","author":{"@type":"Person","name":"Chanho Lee","description":"Îã§ÏñëÌïú Î∂ÑÏïºÎ•º Í≥µÎ∂ÄÌïòÍ≥† ÏûàÎäî ÌïôÏÉù","url":"https://github.com/teddygood","image":"https://github.com/teddygood.png"},"keywords":["orchestration","agent","llm","benchmark","planning"],"isPartOf":{"@type":"Blog","@id":"https://teddygood.github.io/en/blog","name":"Î∏îÎ°úÍ∑∏"}}</script><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="Chanho Lee RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="Chanho Lee Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EFT0SBFJCH"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-EFT0SBFJCH",{})</script>



<link rel="search" type="application/opensearchdescription+xml" title="Chanho Lee" href="/en/opensearch.xml">
<link rel="alternate" type="application/rss+xml" href="/en/projects/rss.xml" title="Chanho Lee Projects RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/projects/atom.xml" title="Chanho Lee Projects Atom Feed">
<link rel="alternate" type="application/json" href="/en/projects/feed.json" title="Chanho Lee Projects JSON Feed">
<link rel="stylesheet" href="/katex/katex.min.css"><link rel="stylesheet" href="/en/assets/css/styles.8fb09425.css">
<script src="/en/assets/js/runtime~main.1a2b5eb9.js" defer="defer"></script>
<script src="/en/assets/js/main.2c52ec55.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo-new.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/logo-new.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Chanho Lee</b></a><a class="navbar__item navbar__link" href="/en/wiki/introduction">Wiki</a><a class="navbar__item navbar__link" href="/en/projects">Projects</a><a class="navbar__item navbar__link" href="/en/blog/archive">Archive</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/blog/OrchestrationBench" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko-KR">ÌïúÍµ≠Ïñ¥</a></li><li><a href="/en/blog/OrchestrationBench" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/teddygood" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><a href="https://www.linkedin.com/in/teddygood/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-linkedin-link" aria-label="LinkedIn Account"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent Posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/en/blog/OrchestrationBench">[Paper Review] OrchestrationBench: LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/Linked-List">Why Linked Lists Are Rarely Used in Python</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/ai-engineering">üìñAI Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/hands-on-llm">üìñHands-on LLM</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/Technical-Interview-Notes-for-Developers">üìñDeveloper Technical Interview Notes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/geultto-Recap">üé¨The first and final post. The end.</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/deep-learning-from-scratch-remastered">üìñDeep Learning from Scratch 1 (Remastered Edition)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/Hands-on-generative-AI-dev-with-AWS-Bedrock">Hands-on Generative AI Development with Amazon Bedrock</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/2024-Recap">‚ú®2024 Reflection</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/blog/Nov-Recap">‚ú®November Reflection</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">[Paper Review] OrchestrationBench: LLM-Driven Agentic Planning and Tool Use in Multi-Domain Scenarios</h1><div class="container_mt6G margin-vert--md"><time datetime="2026-02-04T14:57:00.000Z">February 4, 2026</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/teddygood" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/teddygood.png" alt="Chanho Lee"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/teddygood" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Chanho Lee</span></a></div><small class="authorTitle_nd0D" title="Îã§ÏñëÌïú Î∂ÑÏïºÎ•º Í≥µÎ∂ÄÌïòÍ≥† ÏûàÎäî ÌïôÏÉù">Îã§ÏñëÌïú Î∂ÑÏïºÎ•º Í≥µÎ∂ÄÌïòÍ≥† ÏûàÎäî ÌïôÏÉù</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><blockquote>
<p>ICLR 2026. <a href="https://openreview.net/forum?id=Oljnxmf4pc&amp;noteId=NyL52JeO4j" target="_blank" rel="noopener noreferrer" class="">Paper</a> <a href="https://github.com/kakao/OrchestrationBench" target="_blank" rel="noopener noreferrer" class="">GitHub</a></p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="intro">Intro<a href="#intro" class="hash-link" aria-label="Direct link to Intro" title="Direct link to Intro" translate="no">‚Äã</a></h2>
<p>Recently, services like moltbot (now openclaw) and oh-my-opencode have gained traction, so I thought this paper would be interesting to read and review. This paper was published by Kakao. If you want an easier explanation than my review, check <a href="https://www.kakaocorp.com/page/detail/11919" target="_blank" rel="noopener noreferrer" class="">Kakao&#x27;s announcement (&quot;Kakao&#x27;s self-developed LLM orchestration benchmark paper accepted to ICLR 2026... released as open source on GitHub&quot;)</a>.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="quick-summary">Quick Summary<a href="#quick-summary" class="hash-link" aria-label="Direct link to Quick Summary" title="Direct link to Quick Summary" translate="no">‚Äã</a></h2>
<ol>
<li class="">It introduces a bilingual (Korean and English) benchmark for evaluating LLM orchestration in realistic multi-domain environments.</li>
<li class="">It separates evaluation into workflow planning and tool execution, and uses structured metrics such as Graph Edit Distance (GED).</li>
<li class="">The benchmark contains a manually created dataset across 17 domains and nearly 100 tools, including constraint validation and dynamic revision.</li>
<li class="">Experiments show relatively consistent tool execution but substantial variation in planning, highlighting the need for structured evaluation.</li>
<li class="">The benchmark is designed to be extensible to new domains, tools, and deployment contexts.</li>
</ol>
<!-- -->
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">‚Äã</a></h2>
<p>Most existing benchmarks are either simplified or isolated by domain, so they are insufficient for evaluating the orchestration capability required for service-ready LLMs. To bridge this gap, the paper introduces OrchestrationBench, a bilingual benchmark for realistic service environments. It defines a comprehensive evaluation protocol centered on workflow planning and constraint-aware tool execution. Workflow planning is formalized as workflow construction. Each workflow is represented as a Directed Acyclic Graph (DAG). Constraint-aware tool execution goes beyond the syntactic correctness of tool calling.</p>
<p><img decoding="async" loading="lazy" alt="Architecture" src="/en/assets/images/01-architecture-af5ee1be1d55bf888398574d8ae58af3.png" width="833" height="402" class="centered_bfhO img_ev3q"></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="related-work">Related Work<a href="#related-work" class="hash-link" aria-label="Direct link to Related Work" title="Direct link to Related Work" translate="no">‚Äã</a></h2>
<p>The paper groups prior work into four areas.</p>
<p><strong>Tool Execution Benchmarks</strong><br>
<!-- -->BFCL, API-Bank, T-Eval, and ToolBench evaluate whether agents can decompose tasks and invoke the right tools or APIs.</p>
<p><strong>Single-agent Task Performance Benchmarks</strong><br>
<!-- -->TaskBench, œÑ-bench, GAIA, WebArena, and OSWorld evaluate whether a single agent can complete tasks in specific environments. These benchmarks are valuable because they measure agentic task execution across multiple domains. OrchestrationBench differs by focusing less on individual agent performance and more on LLM-to-LLM collaboration, where a main model orchestrates and invokes specialized LLMs.</p>
<p><strong>Tool Safety Benchmarks</strong><br>
<!-- -->ToolEmu, R-Judge, and SafeToolBench evaluate refusal behavior for risky requests. More specifically, ToolEmu targets single-agent settings, while R-Judge and SafeToolBench focus on multi-turn agents. ToolEmu emphasizes safety alignment of individual tool calls in emulated environments rather than complex multi-step planning. In contrast, OrchestrationBench tests whether the main LLM can reject infeasible requests by correctly understanding the functional descriptions and constraints of available sub-LLMs, evaluating functional feasibility rather than safety alignment itself.</p>
<p><strong>Agentic Planning Benchmarks</strong><br>
<!-- -->These benchmarks evaluate planning and coordination at different abstraction levels. Examples include PlanBench, MultiAgentBench, and REALM-Bench. While they test planning or tool-calling abilities of agents, OrchestrationBench presents a hierarchical LLM-to-LLM orchestration task where the main LLM dynamically coordinates specialized sub-LLMs from natural-language capability descriptions. It also provides diagnostic evaluation by separating workflow planning from constraint-aware execution, which is particularly relevant for production chatbot deployment.</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/02-related-work-7a35f3696610d9073962318e3264a417.png" width="1848" height="578" class="centered_bfhO img_ev3q"></p>
<p>OrchestrationBench is presented as the only benchmark that satisfies all of these dimensions.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-orchestrationbench-framework">The OrchestrationBench Framework<a href="#the-orchestrationbench-framework" class="hash-link" aria-label="Direct link to The OrchestrationBench Framework" title="Direct link to The OrchestrationBench Framework" translate="no">‚Äã</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-the-complexity-of-evaluating-llms-in-real-world-environments">1. The Complexity of Evaluating LLMs in Real-World Environments<a href="#1-the-complexity-of-evaluating-llms-in-real-world-environments" class="hash-link" aria-label="Direct link to 1. The Complexity of Evaluating LLMs in Real-World Environments" title="Direct link to 1. The Complexity of Evaluating LLMs in Real-World Environments" translate="no">‚Äã</a></h3>
<p>Evaluating LLMs in real service environments is more complex than simple QA. Consider the following user request:</p>
<blockquote>
<p>&quot;Book a flight to Seoul, find a hotel near COEX, and share the itinerary with my team.&quot;</p>
</blockquote>
<p>Handling this request requires:</p>
<ol>
<li class=""><strong>multi-step planning</strong>: sharing the itinerary is only possible after reservations are completed</li>
<li class=""><strong>dynamic adaptation</strong>: handling revisions such as &quot;change it to a morning flight&quot;</li>
<li class=""><strong>constraint validation</strong>: detecting system constraints (for example, only on-the-hour or 30-minute slots) and proposing alternatives</li>
</ol>
<p>These scenarios highlight entangled challenges in planning, adaptation, and constraint-aware execution that static benchmarks often fail to capture.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-orchestrationbench-architecture">2. OrchestrationBench Architecture<a href="#2-orchestrationbench-architecture" class="hash-link" aria-label="Direct link to 2. OrchestrationBench Architecture" title="Direct link to 2. OrchestrationBench Architecture" translate="no">‚Äã</a></h3>
<p>OrchestrationBench introduces a comprehensive evaluation framework with bilingual datasets in English and Korean.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="21-advanced-planning-and-coordination">2.1 Advanced Planning and Coordination<a href="#21-advanced-planning-and-coordination" class="hash-link" aria-label="Direct link to 2.1 Advanced Planning and Coordination" title="Direct link to 2.1 Advanced Planning and Coordination" translate="no">‚Äã</a></h4>
<p>As shown below, orchestration is formalized as a structured workflow schema that defines each task&#x27;s execution status, dependency relations, and step-level planning. This structure allows evaluation of whether models can manage sequential and parallel execution, handle inter-workflow dependencies, and adapt to user interactions during execution.</p>
<p><strong>Workflow Schema</strong></p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/03-workflow-schema-64ee027002454de7fd9bcd2ccdb9b7c0.png" width="1422" height="324" class="centered_bfhO img_ev3q"></p>
<p><strong>Example</strong>: &quot;I have a business trip to Seoul tomorrow. Book my flight, find a hotel near COEX, and share my itinerary with my team.&quot;</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">workflow_1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> pending</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> independent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">steps</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> pending</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> travel_agent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">refined_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Book a flight for tomorrow&#x27;s business trip to Seoul&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">workflow_2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> pending</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> independent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">steps</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> pending</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> travel_agent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">refined_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Book a hotel near COEX&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">workflow_3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> pending</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> dependent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">depend_on</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;workflow_1&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;workflow_2&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">steps</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> pending</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> calendar_agent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">refined_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Share the completed itinerary with my team&quot;</span><br></span></code></pre></div></div>
<p><code>depend_on</code> specifies which workflows must finish before the current workflow starts. Since <code>workflow_3</code> depends on <code>workflow_1</code> and <code>workflow_2</code>, it can run only after both are completed.</p>
<p>In real environments, user queries often evolve dynamically rather than following a fixed plan. OrchestrationBench evaluates whether models can adjust flexibly by creating new workflows when additional tools are needed, and splitting workflows when explicit confirmation or branching into sub-tasks is required. For example, when a user revises an existing request or adds conditions mid-conversation, the model should update or extend workflows while maintaining consistency with prior steps. Clear criteria define when workflows should be split or merged. Independent requests (for example, requesting both flight planning and hotel recommendations) or tasks requiring intermediate confirmation (for example, approval before booking) are handled as separate workflows. By contrast, tasks that are coherent and contribute to a single goal are kept within one workflow.</p>
<p>In short, this framework evaluates not only planning, but also a model&#x27;s ability to adjust, pause, and resume workflows in interactive scenarios.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="22-comprehensive-tool-use">2.2. Comprehensive Tool Use<a href="#22-comprehensive-tool-use" class="hash-link" aria-label="Direct link to 2.2. Comprehensive Tool Use" title="Direct link to 2.2. Comprehensive Tool Use" translate="no">‚Äã</a></h4>
<p>Tool execution evaluation goes beyond simple tool-call accuracy and covers the full service-level interaction flow.
It evaluates not only whether the model invokes tools correctly, but also when tool use is needed, when direct responses are sufficient without tools, and whether the model can recognize insufficient or ambiguous user input and proactively ask for clarification. This behavior is represented by the <code>AWAIT_FOR_USER_INPUT</code> signal.</p>
<p>Beyond syntactic correctness, real services must strictly enforce domain-specific business rules. Before actually invoking a tool, the model performs pre-execution validation, and emits <code>TOOL_CONSTRAINT_VIOLATION</code> when required constraints are not satisfied.</p>
<ul>
<li class="">Maintaining logical consistency<!-- -->
<ul>
<li class="">Checking whether the user request is logically coherent and non-contradictory</li>
<li class="">e.g., a return date earlier than a departure date in flight booking</li>
</ul>
</li>
<li class="">Enforcing resource constraints<!-- -->
<ul>
<li class="">Checking whether the request respects resource limits provided by a service or tool</li>
<li class="">e.g., ordering beyond available inventory, or purchases exceeding budget limits</li>
</ul>
</li>
</ul>
<p>After passing validation on user requests and extracted information, the LLM should execute the external tool with correctly formatted parameters.</p>
<p>Model performance is measured through call/reject classification metrics. <code>AWAIT_FOR_USER_INPUT</code> and <code>TOOL_CONSTRAINT_VIOLATION</code> are reject cases, while successful execution is assessed with function-calling metrics.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="23-multi-domain-tool-environments">2.3. Multi-Domain Tool Environments<a href="#23-multi-domain-tool-environments" class="hash-link" aria-label="Direct link to 2.3. Multi-Domain Tool Environments" title="Direct link to 2.3. Multi-Domain Tool Environments" translate="no">‚Äã</a></h4>
<p>The benchmark defines 17 representative service domains that stay independent from specific services while remaining extensible to real applications. Each domain is built around realistic but generalized scenarios so that a model&#x27;s orchestration and instruction execution capability can be evaluated.</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/04-multi-domain-tools-341eec496f80d6671ee5a8ceb6d6fa2d.png" width="1548" height="1646" class="centered_bfhO img_ev3q"></p>
<p>There are 97 tools in English and 99 in Korean. Korean includes additional culture-specific tools such as address romanization and fortune-related information. Unlike prior benchmarks that use simplified tool abstractions, OrchestrationBench integrates domain-specific constraints and realistic behavior to provide fine-grained coverage of diverse tasks and better simulate real service environments.</p>
<p>Collectively, these domains represent three common user workflow types. This categorization emphasizes that the benchmark primarily reflects everyday consumer services while remaining extensible to utility and productivity contexts.</p>
<ol>
<li class="">inquiry and information tasks (e.g., weather checks, place search, reading news)</li>
<li class="">action and transaction tasks (e.g., booking flights, purchasing items)</li>
<li class="">planning and coordination tasks (e.g., scheduling meetings, sending messages, arranging delivery)</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-dataset-construction">3. Dataset Construction<a href="#3-dataset-construction" class="hash-link" aria-label="Direct link to 3. Dataset Construction" title="Direct link to 3. Dataset Construction" translate="no">‚Äã</a></h3>
<p>The dataset is designed to capture the complexity and realism of real-world service orchestration. To ensure authenticity and quality, all conversation sessions, workflows, and tool calls are manually created by trained annotators following detailed construction guidelines, rather than being artificially generated. This ensures that dialogue flow, tool usage, and constraint handling faithfully reflect realistic user-service interactions rather than model-generated synthetic patterns.</p>
<p>The table below shows the overall construction pipeline, including domain selection, virtual tool design, and manual review/validation. Every scenario is cross-validated by at least three annotators to ensure consistency and accuracy. To enable controlled and interpretable evaluation, ambiguous or multi-solution cases are excluded, and data is built from tasks with clear, well-defined dependencies. As a result, the benchmark achieves high reliability while remaining independent of any single model or proprietary API.</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/05-dataset-construction-f999b66fab602f353e89a6fb9e59a7cd.png" width="1534" height="654" class="centered_bfhO img_ev3q"></p>
<p><strong>Scenario Type Examples</strong></p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/06-scenario-types-9e6eff6c1979b75e68ae64aa69323400.png" width="1488" height="534" class="centered_bfhO img_ev3q"></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-dataset-scale-and-distribution">4. Dataset Scale and Distribution<a href="#4-dataset-scale-and-distribution" class="hash-link" aria-label="Direct link to 4. Dataset Scale and Distribution" title="Direct link to 4. Dataset Scale and Distribution" translate="no">‚Äã</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/07-dataset-scale-079b5467c508641d73dc4df84879991c.png" width="888" height="228" class="centered_bfhO img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/08-dataset-distribution-ea6733044191206ba0ebe814c0a555b5.png" width="1436" height="1436" class="centered_bfhO img_ev3q"></p>
<p>This dataset includes both English and Korean subsets with similar scale. Both cover 17 representative service domains and intentionally show asymmetric tool distributions. Broad domains such as Places or Entertainment include many tools, while narrower domains like Weather or News remain compact to reflect realistic usage frequency.</p>
<p>At the workflow level, most sessions include 2-3 workflows across 2-3 domains, while some expand to up to 7 steps or span four or more domains. This shows that a single session typically requires multiple planning rounds, with some including up to seven planning steps. The frequent inclusion of two or more domains reflects realistic multi-domain scenarios where users switch across heterogeneous services. In terms of tool invocation, sequential and parallel call structures dominate over isolated single calls, indicating the complexity of orchestration needed to complete real tasks.</p>
<p>Taken together, these distributions show that OrchestrationBench covers a broad range of real orchestration patterns and enables fine-grained evaluation of planning, tool invocation, and adaptive reasoning capabilities. This highlights that the benchmark goes beyond isolated QA or simple tool calling, enabling realistic orchestration assessment in constraint-aware service environments.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="evaluation">Evaluation<a href="#evaluation" class="hash-link" aria-label="Direct link to Evaluation" title="Direct link to Evaluation" translate="no">‚Äã</a></h2>
<p>Current end-to-end benchmarks such as AgentBench, GAIA, SWE-bench, and œÑ-bench provide flexibility but often obscure failure points in complex multi-step tasks. To address this limitation, the paper uses step-wise evaluation that separates and tests each component independently. The evaluation distinguishes two major stages: Planning and Tool Execution. To capture subtle behaviors of sub-LLMs, tool execution is further decomposed into two sequential criteria: call/reject classification accuracy and function-calling performance.</p>
<p><strong>Models</strong><br>
<!-- -->All reasoning models are configured with low reasoning effort settings.</p>
<ul>
<li class="">OpenAI GPT Models (gpt-4.1, gpt-4o, gpt-5)</li>
<li class="">Anthropic Claude Models (claude-sonnet-4)</li>
<li class="">Google Gemini Models (gemini-2.5-pro-preview, gemini-2.5-flash-preview)</li>
<li class="">Alibaba Qwen Models (Qwen3 series)</li>
<li class="">Korean Open-Source Models (SKT A.X-4.0, Kakao kanana-1.5, LG AI Research EXAONE-4.0)</li>
</ul>
<p><strong>Evaluation Protocol</strong></p>
<ul>
<li class="">Each target LLM receives the full dialogue history up to the evaluation point.</li>
<li class="">Parallelly executed LLMs operate on separated histories to prevent information leakage.</li>
<li class="">Sequentially executed LLMs access cumulative dialogue history including prior model outputs.</li>
<li class="">Workflow generation by the main LLM is triggered only by user input.</li>
<li class="">Sub-LLMs process refined queries forwarded by the main LLM and additional clarifications provided by users.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-evaluation-metrics">1. Evaluation Metrics<a href="#1-evaluation-metrics" class="hash-link" aria-label="Direct link to 1. Evaluation Metrics" title="Direct link to 1. Evaluation Metrics" translate="no">‚Äã</a></h3>
<p>The evaluation is split into two parts. This separation is important because a model can plan well but fail in execution, and conversely, good call-level accuracy does not guarantee robust end-to-end workflow behavior.</p>
<p><strong>Planning Assessment</strong>
To evaluate workflow generation quality, the paper uses Graph Edit Distance (GED). GED quantifies structural differences by counting the minimum edit operations required to transform one graph into another, following the method in Advancing Agentic Systems. In this paper, 1-GED is used so that higher values indicate better performance. The workflow representation includes workflow structure, step assignment (sub-LLM selection), and execution status. The framework performs hierarchical workflow scoring: structural score measures workflow topology correctness, and component score evaluates step-level assignments. Error weighting gives higher weight to selection errors (0.8) than status errors (0.2), reflecting the intuition that selecting the wrong tool is generally more harmful to task success than misjudging execution status. These weights are not empirically derived.</p>
<p><strong>Tool Execution Assessment</strong>
To evaluate tool execution comprehensively, two core components are analyzed: (1) the model&#x27;s ability to make correct call/reject decisions, and (2) the quality of actual function execution. Call/reject classification accuracy measures the ratio of correct decisions over all cases, including both proper rejections and successful function-call attempts. Function-calling performance evaluates function-call correctness only among cases that correctly proceed to the function-call stage, using three detailed metrics: tool selection F1, key F1, and argument F1.</p>
<p>Function-calling parameter validation uses a three-stage approach:</p>
<ol>
<li class="">Exact match comparison</li>
<li class="">Type/pattern validation based on tool descriptions</li>
<li class="">Semantic validation for remaining cases</li>
</ol>
<p>To reduce model bias, the paper uses an ensemble of three LLM judges - GPT-4.1, Claude Sonnet 4, and Gemini 2.5 Flash - with temperature 0.3, aggregating scores by arithmetic mean.</p>
<p>The LLM judge classifies true/false positives and negatives, and these decisions are integrated into F1 calculations. To further ensure reliability, inter-rater agreement between annotators and the LLM judge is measured, yielding a Cohen&#x27;s Kappa of 0.63, which indicates substantial agreement. To maintain compatibility with function-calling training (JSON output format), call rejections and information requests are implemented in XML output format.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-evaluation-results">2. Evaluation Results<a href="#2-evaluation-results" class="hash-link" aria-label="Direct link to 2. Evaluation Results" title="Direct link to 2. Evaluation Results" translate="no">‚Äã</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/09-evaluation-results-51393a28ec55f6bae8a2806d85655e7a.png" width="1614" height="1522" class="centered_bfhO img_ev3q"></p>
<p>Best performance is shown in bold and second-best performance is underlined. Claude was evaluated via AWS Bedrock using <code>anthropic.claude-sonnet-4-20250514-v1:0</code>. Workflow score is computed as 1-GED, where higher is better.</p>
<p><strong>Key Findings</strong></p>
<p><strong>Open-Source Model Viability</strong></p>
<p>Open-source dense models such as Qwen3-235B-A22B achieved performance close to proprietary models. Dense architectures also consistently outperformed Mixture-of-Experts (MoE) variants on planning tasks.</p>
<p><strong>Model-Specific Specializations</strong></p>
<p>Gemini showed the best planning performance (English 0.850) but relatively weaker function calling. Claude-sonnet-4 showed the best function-calling performance (English 0.885). GPT-4.1 showed balanced performance. In particular, workflow generation showed relatively large gaps between top-tier and lower-performing models in both English and Korean, highlighting planning as the most discriminative capability among evaluated tasks.</p>
<p><strong>Planning-Execution Gap</strong></p>
<p>The correlation between Planning and Call/Reject is relatively low (English 0.58, Korean 0.45). This indicates a weaker link between planning and decision outcomes. In other words, models can generate good workflows but still make inaccurate execution decisions.</p>
<p><strong>Language-Dependent Performance</strong></p>
<p>Claude&#x27;s English Call/Reject decision score is 0.868 but drops to 0.759 in Korean. Gemini, by contrast, is stronger in Korean. This suggests language-specific training effects and reinforces the importance of bilingual evaluation.</p>
<p>These results show that despite strong individual capabilities, the planning-execution gap can limit real-world agentic performance, emphasizing the need for task-specific and language-aware model selection.</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/en/assets/images/10-metric-correlation-75d54e15803c1f3175d5c0c1986dd2bc.png" width="1358" height="426" class="centered_bfhO img_ev3q"></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion-and-future-works">Conclusion and Future Works<a href="#conclusion-and-future-works" class="hash-link" aria-label="Direct link to Conclusion and Future Works" title="Direct link to Conclusion and Future Works" translate="no">‚Äã</a></h2>
<p>OrchestrationBench is the first bilingual (English/Korean) benchmark for evaluating LLM orchestration capabilities in realistic multi-domain service environments. By separating orchestration into workflow planning and tool execution, it provides detailed insights into model performance across different aspects of agentic reasoning.</p>
<p>There are two main conclusions:</p>
<ul>
<li class="">Workflow planning shows much larger model gaps than function calling, so careful model selection is critical for orchestration tasks.</li>
<li class="">While models execute function calls relatively well, they struggle with call/reject classification, i.e., deciding whether function calling is appropriate under real-world tool constraints.</li>
</ul>
<p>Limitations are as follows:</p>
<ul>
<li class="">The 17 domains may not fully capture all orchestration scenarios or generalize to other languages.</li>
<li class="">The use of virtual tools does not reflect the full complexity of real API integration.</li>
<li class="">Turn-by-turn evaluation assumes successful execution at each step, which can overestimate overall performance metrics.</li>
</ul>
<p>Future directions include:</p>
<ul>
<li class="">Real multi-domain tool integration through frameworks such as MCP</li>
<li class="">Training methods to reduce the planning-execution gap</li>
<li class="">Support for more sophisticated multi-agent collaboration patterns</li>
</ul></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/en/blog/tags/paper-review">Paper Review</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/en/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/en/blog/tags/agent">Agent</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/teddygood/teddygood.github.io/tree/main/blog/2026-02-04-OrchestrationBench/2026-02-04-OrchestrationBench.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/en/blog/Linked-List"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Why Linked Lists Are Rarely Used in Python</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#intro" class="table-of-contents__link toc-highlight">Intro</a></li><li><a href="#quick-summary" class="table-of-contents__link toc-highlight">Quick Summary</a></li><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#related-work" class="table-of-contents__link toc-highlight">Related Work</a></li><li><a href="#the-orchestrationbench-framework" class="table-of-contents__link toc-highlight">The OrchestrationBench Framework</a><ul><li><a href="#1-the-complexity-of-evaluating-llms-in-real-world-environments" class="table-of-contents__link toc-highlight">1. The Complexity of Evaluating LLMs in Real-World Environments</a></li><li><a href="#2-orchestrationbench-architecture" class="table-of-contents__link toc-highlight">2. OrchestrationBench Architecture</a></li><li><a href="#3-dataset-construction" class="table-of-contents__link toc-highlight">3. Dataset Construction</a></li><li><a href="#4-dataset-scale-and-distribution" class="table-of-contents__link toc-highlight">4. Dataset Scale and Distribution</a></li></ul></li><li><a href="#evaluation" class="table-of-contents__link toc-highlight">Evaluation</a><ul><li><a href="#1-evaluation-metrics" class="table-of-contents__link toc-highlight">1. Evaluation Metrics</a></li><li><a href="#2-evaluation-results" class="table-of-contents__link toc-highlight">2. Evaluation Results</a></li></ul></li><li><a href="#conclusion-and-future-works" class="table-of-contents__link toc-highlight">Conclusion and Future Works</a></li></ul></div></div></div></div></div></div>
</body>
</html>